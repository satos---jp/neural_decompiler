\documentclass[senior,final,11pt]{iscs-thesis}
%論文の種類とフォントサイズをオプションに
%\usepackage{graphicx}% 必要に応じて
%\usepackage{mysettings}% 自分用設定
%-------------------
\etitle{Neural Network based Decompiler for Machine Coding}
\jtitle{ニューラルネットを用いた機械語のための逆コンパイラ}

__NAMES__

\date{December 11, 2018}
%-------------------


\newcommand{\argmax}{\mathop{\rm arg\,max}\limits}
\newcommand{\argmin}{\mathop{\rm arg\,smin}\limits}

\usepackage{listings}
\usepackage{cite}
\usepackage{url}
\usepackage{natbib}

\lstdefinestyle{myCustomMatlabStyle}{
  language=C,
  numbers=none,
  stepnumber=1,
  numbersep=10pt,
  tabsize=2,
  showspaces=false,
  showstringspaces=false
}


\lstset{%
  basicstyle={\small},%
  commentstyle={\small\itshape},%
  keywordstyle={\small\bfseries},%
  % style={myCustomMatlabStyle},
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},%
  numbers=left,%
  xrightmargin=0zw,%
  xleftmargin=3zw,%
  numberstyle={\scriptsize},%
  stepnumber=1,
  numbersep=1zw,%
  lineskip=-0.5ex,%
  captionpos=b
}



\begin{document}
\begin{eabstract}
A decompiler is a tool for recovering a source code from compiled binary data.
There are various decompilers, but they often output a source code that is not intelligible to humans. 
In this thesis, we apply machine translation techniques to generate human intelligible decompiled source codes. 
More specifically, we propose to use recurrent neural networks with attention, 
which have been demonstrated to be useful for machine translation. 
In experiments, we use source codes collected from open source projects and their binary data for training and evaluating the neural networks.
(TODO)実験結果について述べること(まだ未確定)
\end{eabstract}
\begin{jabstract}
逆コンパイラはコンパイル後のバイナリデータからソースコードを復元するためのツールである。
様々な逆コンパイラが存在するが、既存の逆コンパイラはしばしば人間にとって分かりにくいソースコードを出力する。
そこで本論文では、統計的機械翻訳の技術を逆コンパイラに用いた、解析者にとってわかりやすいソースコードを出力する逆コンパイラを提案する。
具体的には、機械翻訳において有用とされる注意機構付き再帰型ニューラルネットワークを用いる。
実験では、オープンソースプロジェクトから収集したソースコードとそのバイナリデータを用いてニューラルネットワークを学習し、その性能を検証した。
\end{jabstract}

\maketitle

\chapter{Introduction}
% For dinamic analysis, malware is executed in the sandbox and the behavior is examined with the changes ocuuring in the sandbox. 
% Usually, there are two malware analysis approach, the dinamic analysis and the static analysis.
% In the static analysys, the behavior of the malware is analyzed by 

These days, the amount of malware has been increasing rapidly.

要参考文献
So the importance of developing malware analysis tools has been rising.
要参考文献
One of these tools is a decompiler, which is a tool for analyzing the behavior of malware. 
This tool generates a C-like pseudocode which represents the behavior of the malware, and analysts can infer the behavior of the malware from the pseudocode.
This is much easier compared to analyzing raw binary codes.

However, sometimes a decompiler generates an unstructured code, which requires users to make an effort to understand the behavior of the program. 
This is due to flexibility of the high level language. If the decompiler chooses a human-friendly representation of the language as a result of decompiling, we can analyze malware with less effort. 

In this thesis, we apply neural networks to the problem of decompilation to generate human-friendly pseudocodes. 
This idea is originated from \cite{Motoneta}, in which they used recurrent neural networks.
They used sequence-to-sequence model, which ignores the structure of the C language code. 
Therefore, we implemented sequence-to-tree model, which is suggested in \cite{Seq2Tree}, for generating structured C language source code 
結果が向上していれば(and improving the decompilation accuracy.)といいたい。

従来法の何が悪いのかを説明する．たぶんした。
(TODO)このあとに今回やる更に具体的な内容とその成果についてが入る。

\section{Related Works}
In this section, we review related works.

\subsection{Decompilation with Machine Learning}

Because perfect decompilation is naturally impossible, there are many machine learning applications for decompilation.

In \cite{Motoneta}, they used the seqence-to-sequence approach, which is a popular technique in the machine translation.

In \cite{genetic_decompiler}, they applied genetic programming to decompilation.
They prepared big corpus of C source codes, and they repeatedly mutated C code to reduce the difference of the target binary and the compilation result of the source code. 



% Compiler and Optimization Level Estimation for Improving Anti-malware Technologies

% 

\subsection{Improvement of decompilation results}
In this section, we do not try to recover the function name and variable name. 
However, for hand-decompilation, sometimes the name can be estimated by the functionality of the variable.
要参考文献
For example, assume there is a variable X which seems to have a C struct type and has two fields P and Q.
Also, there are two functions F and G which operate with X. 
The function F receives X and another value V, assigns V to the memory indicated by P, adds 8 to P and increases Q. 
The function G receives X, subtracts 8 from P, decreases Q, and returns the value indicated by P.
In the situation, is is estimated that X has a name related to {\sl stuck}, the function F has a name related to {\sl push},
ands G has a name related to {\sl pop}.

\cite{name_recover_from_decompile_result} tackled this work. 
In that paper, they tried to estimate the name of a variable which appears in the decompilation result of a compiled C program.

This kind of name estimation also appears in the context of deobfuscation.
要参考文献
Deobfuscation is a reverse process of obfuscation.
Obfuscation is a technique to modify the program source code so that the code becomes hard to understand for humans. 
Obfuscation is used for hiding the behavior of a program whose source code has to be distributed.
For example, in the web application, a JavaScript code of a program should be distributed in order to be executed on client web browsers.
To protect the program from cracking, the JavaScript code is sometimes minified. All comments, newlines, indents are removed and variable names are substituted to automatically generated meaningless names.

\cite{JSNaughty} applied statistical machine translation for this task. 
They calculated estimated the name of the variable by the functionality of it

\subsection{Neural Network for Program Generation}

There are many applications of neural networks for program synthesis. They are summurized in \cite{deep_programming_matome}.

In this thesis, we applied seq-to-tree model suggested in \cite{Seq2Tree}, because the input of the decompiler is the sequence of machine codes.

For another example, in \cite{coffeescript_to_javascript}, they tried to mutually translate from JavaScript to CoffeeScript, and its inverse.
In that case, both JavaScript codes and CoffeeScript codes are tree structured, so they suggested tree-to-tree model.


\chapter{Problem Settings and Methots}

In this chapter, we explain about the statistical formalization of the decompilation problem,
and the neural network models used in the experiment.
\section{Decompiler}

% 320 words.

A compiler is a program which compiles source code written in language X to other language Y. 
Usually, X is high-level language and Y is relatively low-level language. 
For example, clang compiler translates C language to x64 assembly language, javac compiler translates Java to Java virtual machine code.

A decompiler is a program which aims to reverse the process of a compiler, retrieve a source code of the high-level language X from the source code of the low-level language Y. 
This reversing process is usually insufficient or impossible.
This is because many informaition, such as variable name or function name, are lost when compiling.
Additionally, the program structure described in the high level language are lost too. 
For example, a simple {\sl for statement} can be represented by the combination of a {\sl while statement} and an {\sl if statement}, or the combination of {\sl if statement} and {\sl goto statement}. 
So a decompiler can't distinguish their representation difference, despite in the real situation {\sl for statement} are often used and {\sl goto statement} are rarely. 

Existing decompilers are made up with many steps similar to compilers. 
They find patterns in binary data, convert them to more high-level structure, and chain them so that they finally generate high-level psudecode.
Their output psudecodes are sometimes wrong or hard to understand.

% https://github.com/torvalds/linux/blob/70c25259537c073584eb906865307687275b527f/lib/rbtree.c#L528


% (DONE)表がはみ出しているのでどうにかした。

\begin{figure}
	\begin{tabular}{cc}
		\begin{minipage}[c]{0.5\hsize}
			\lstinputlisting[title={sample C source code},breaklines=false]{bintree.c}
		\end{minipage}
		\\
		\begin{minipage}[c]{0.5\hsize}
			\lstinputlisting[title={result of the Hopper Decompiler}]{hopper.c}
		\end{minipage}
		\begin{minipage}[c]{0.5\hsize}
			\lstinputlisting[title={result of the Snowman Decompiler}]{snowman.c}
		\end{minipage}
	\end{tabular}
	\vspace*{-0.6cm}
	\caption{ decompilation results of existing decompilers for C source code}
	\label{fig:cw}
\end{figure}


Figure~\ref{fig:cw} shows the example decompilation results of the Hopper disassembler and the snowman decompiler 
for a sample C language code.
The Hopper decompiler fails to detect while statements, and the variable {\sl now} is incorrectly analyzed.
The output of snowman seems rather correct, as type {\sl struct tree} is reconstructed to {\sl struct s0}, 
and the {\sl if} statement and {\sl while} statement are correctly restored. 
However, it fails to detect the function epilogue, and the {\sl free} function call is incorrectly analyzed.
Ideally, the decompilers can find correct function ranges and structure of statements, but they failed.


\section{Problem Settings}

% 189
In this paper, we try to apply statistical approach, specifically SMT (statistical machine translation) technique, for decompilation.
The decompilation problem is formulated as follows. 

Given $sx = [sx_1, \dots, sx_n] $ as a tokenized sequence of the source code of domain language X, compiler generates $sy$ as a code of target language Y with probability $p(sy|sx)$.
Then, the best decompilation result for target source code $sy$ is $ \argmax_{sx} p(sx|sy)$. 
To generate $ \argmax_{sx} p(sx|sy)$, we decomposit $ p(sx|sy) = p([sx_1, \dots, sx_n] |sy) = \prod_{k=1}^{n} p(sx_k|sy,[sx_1,\dots,sx_{k-1}]) $,
where $ p(sx_k|sy,[sx_1,\dots,sx_{k-1}]) $ represents the probability that the token $ sx_k $ follows to the sequence $ [sx_1,\dots,sx_{k-1}] $ 
when the input sequence is $ sy $. 
We try to estimate the probability $ p(sx_k|sy,[sx_1,\dots,sx_{k-1}]) $ by LSTM, which is described in the next section.
With estimated $ p(sx_k|sy,[sx_1,\dots,sx_{k-1}]) $, we can generate approximated $ \argmax_{sx} p(sx|sy)$ sequentially from $sx_1$ to $sx_n$ by selecting token $ \argmax_{sx_{k}} p(sx_k|sy,[sx_1,\dots,sx_{k-1}]) $ at each timing.

% The LSTM is 
% We assume the prior distribution $p(sx)$ as the human-generated source code. 
% With that assumption, is in proportion to $p(sy|sx)p(sx)$ according to the Bayesian low, and 
% the $\argmax_{sx} p(sx|sy)$ is considered as the best human-intelligible decompilation result for the low-level code $sy$.  
% So we try to estimate $ \argmax_{sx} p(sy|sx)p(sx)$ for decompilation result.
% And if the compiler is deterministic, there would be a function $f$ which represents the compiler and $p(f(sx)|sx) = 1$,
% then the objective becomes to $ \argmax_{sx,f(sx)=sy} p(sx)$.

% The distribution $p(sx)$ is approximated by collecting source code from open source projects.
% So if we had enough high-level source code data, we could choose the most popular high-level source code which generates given low-level code for the decompile result.
% However, high-level source code data can't be comprehensive.
% So we have to model the structure of compilation process somehow.
% We modeled the structure by the LSTM, which is commonly used in resent SMT techniques.

\section{Modeling}

% (TODO)LSTM,Seq2seqの説明、その後Attentionについて説明、さらにその後seq2treeについて説明する。
\subsection{LSTM}
%72 words.
LSTM (Long Short-Term Memory) is a unit of Recurrent Neural Network. 
It is first introduced in \cite{first_LSTM}, for overcoming the vanishing gradient problem and exploding gradient problem.
Roughly speaking, LSTM unit recieves two input vectors, 
the one represents the internal state of the LSTM, and the other is the input for the LSTM unit, 
and LSTM emits one output vectors, which represents the next internal state of the LSTM.

\subsection{Seq-to-Seq}
% This model is called seq2seq (sequence to seqence) model.

% 477 words.
LSTM unit is applied for SMT task in \cite{seq2seq}, as the essential unit of the seq-to-seq (sequence to sequence) model.
SMT is the task which tries to translate one sentense $sx$ written in one language into the sentense $sy$ written in another language.

Seq-to-seq model is composed of two modules, the encoder netwrok and the decoder network.

The purpose of the encoder network is to summerize the input sequence $sx$, whose length varies depending on the sequence, into one fixed length vector.
This process is carried out as follows. 
First, $sx$ is tokenized to sequence $sx_{1} \dots sx_{n}$, and each token $sx_{k}$ is embedded to its vector representation $vx_{k}$. 
Then, the initial LSTM internal state $enc_0$ is intialized randomely, 
and $ enc_{1} \dots enc_{n} $ are calculated in accordance with the recursive formula $ enc_{k} = lstm_{enc}(enc_{k-1},vx_{k}) $.
Finally, the last internal state $ enc_{n}$ is returned as the reduced representation of the input sequence $sx$.

The purpose of the decoder network is to estimate the distribution $ p(sy_k|sx,[sy_1,\dots,sy_{k-1}]) $. 
The training process is carried out as follows. 
First, $sy$ is tokenized, appended the token $ EOS $ for indicating the end of the sequence, then embedded to a vector representation $vy_{1} \dots vy_{m}$ in the same way as embedding $sx$.
From the start vector $y_{begin}$ and encoded input $ enc_{n} $, first internal state $ dec_{1}$ is calcuated as $ LSTM_{dec}(enc_{n},y_{begin}) $.
Then, $ dec_{1} \dots dec_{m} $ are calculated in accordance with the recursive formula $ dec_{k} = lstm_{dec}(dec_{k-1},vy_{k}) $.
Each decoder internal state $ dec_{k} $ is passed to the linear layer $ W_{dec} $ and then softmaxed to generate a probability vector $ py_{k} $.
The probability vector $ py_{k} $ is intended to represent the probability distribution $ p(sy_k|sx,[sy_1,\dots,sy_{k-1}]) $.
So the network is trained to reduce the difference between predicted distribution $ py_{k} $ and actual distribution $ p(sy_k|sx,[sy_1,\dots,sy_{k-1}]) $.
In the experiment, we choiced cross-entropy of $ py_{k} $ and $ p(sy_k|sx,[sy_1,\dots,sy_{k-1}]) $ for loss function.

The inference for sequence $sx$ is carried out by calculating 
$ \argmax_{sy} p(sy|sx) = \argmax_{sy} \prod_{k=1}^n p(sy_k|sx,[sy_1,\dots,sy_{k-1}]) $.
Calculating argmax directly is time consuming, so we have to approximate the argmax by calculating $sy = [sy_1, \dots sy_m]$ sequentially.
First, $sy_1$ is calculated as $ \argmax_{sy_1} p(sy_1|sx,[]) $, where distribution $ p(sy_1|sx,[]) $ is approximated by 
$ softmax(W_{dec}(dec_{1}))$. 
Then, the next token $sy_2 = \argmax_{sy_1} p(sy_2|sx,[sy_2]) $ is calculated from $ dec_{1} $ and the vector embedding of $sy_1$, and so on.
When the token $sy_{m+1}$ is the $ EOS $ token, generating process is stopped and the sequence $ [sy_1, \dots sy_m]$ is returned as the inference result.

% (TODO) このへんで 図を入れる
% (TODO) 理由/気持ち について述べたいですね。



% (TODO) BiLSTMとmulti-layerについてさらっと


\subsection{Attention}
Attention is a extension module of seq-to-seq model to improve the quality of translation. 
Simple seq-to-seq model has a problem that the length of the encoded vector $enc_{n}$ is fixed 
regardless of the length of the input sequence $sx$. 
Therefore, the longer the length of the input becomes, the more likely the information of the input will be lost.
This problem is eased by using the information in the encoding network when decoding the output sequence.

The attention module is first developed in \cite{attention_paper}.
However, the attention 
We inplemented the global general attention, which is intoduced in \cite{dot_attention}, for improving the quality of decompilation.
In this subsection, we will describe about the global general attention.



After generating $ dec_{k} $, the context vector $ c_{k} $ is calculated from the hidden states $ enc_{1} \dots enc_{n} $.

$ c_{k} = \sum_{k=1}^{n} $ 

The attentional hidden state $ dec'_{k} $ is calculated from the $ dec_{k} $ and $ c_{k} $.
Then, the probability $ p(sy_k|sx,[sy_1,\dots,sy_{k-1}]) $ is estimated as $ softmax(W_{dec}(dec'_{k}))$, instead of $ softmax(W_{dec}(dec_{k}))$.




\subsection{Seq-to-Tree}
% 184 words
Compare to natural languages, programming languages obeys rigid grammer, wich is usually context free grammer,  
and it is reasonable to consider the grammer of the programming language in generating programs.
Seq-to-Tree is a model for generating tree-structured output from sequence input. This model is introduced in \cite{Seq2Tree} for generating programs from the program specification written in natural languages. 
This task is similer to our task, which aims to generate programs from the sequence of the assembly code, so we implemented this model in the experiment.

Now, we will describe about the seq-2-tree model.
seq-2-tree model is composed of two modules, the encoder netwrok and the decoder network.
The encoder network of seq-2-tree model is the same as the encoder network of seq-2-tree model. It summerize the input sequence into one fixed length vector.

The decoder network is a network for generating tree structured output. 
In this paper, our implemented model is based on model in \cite{Seq2Tree}. 
So first, we briefly describe the model implemented in the paper, and then, we describe our model.

\subsubsection{Previous Model}
% 312 words
In algorithmic parspective, the inference of the seq2seq model tries to generate sequence one by one, from the first token to the last.
% The Seq2Tree model also tries to generate tree structure one by one, with depth-first order.
The inference of the SeqTree model starts from the start symbol, and recursively expand the nonterminal symbols in depth-first order.

The tree-structure is representable by the sequence of the actions which expands a nonterminal symbol of the constructing tree.
Suppose the tree $ty$ is generated by the action sequence $ [a_1, \dots a_m] $, 
where $ a_k $ is the action which expands left-most symbol of the nonterminal symbol in the tree generated by the action sequence $ [a_1, \dots a_{k-1}] $. 
Then, the tree-generating probability $ p(ty|sx) $ is decomposed to $ \prod_{k=1}^n p(a_k|sx,[a_1, \dots a_{k-1}]) $, 
so the seq-2-seq model is now applicable for generating tree structure, by emmbeding each action to representing vector $av_k$ and 
calculating $ LSTM_{dec}(dec_{k},av_{k}) $ for estimating $p(a_{k+1}|sx,[a_1, \dots a_{k}]) $.  

In the paper, they additionaly tries to use the information which is peculiar to the tree structure. 
For expanding the node $d$, they also use the information of node type $ nf_{d} $ and the parent action $p_{d}$ from which $d$ is generated.
And they calculalte $ LSTM_{dec}(dec_{k},[av_{k}; nfv_{d}; pv_{d}; ctx_{k}]) $ instead of $ LSTM_{dec}(dec_{k},av_{k}) $, 
where $nfv_{d},pv_{d}$ are the vector embedding of $nf_{d},p_{d}$, and $ctx_{k}$ is the attention at k.

They also used devised method for generating terminal symbols, because they need to copy the name of some variables from the program specification.
But in our case, we don't need copy the information from the input, because the name information is lost when compiling, so we omit the discriptionp of this copy method.


\subsubsection{Our Model}

Calculating context With 
We changed previous model 

Our model tries to exploit the intuitive that each node of the tree is basically independent from its sibling.
This intuition is not true when the compiler optimizes the assembler. 
However, in this paper, we tested the 









\chapter{Experiments}
In this chapter, we explain the detail of the data preparetion and implementation.
The implementation source codes are avaiable at \url{https://github.com/satos---jp/neural_decompiler}.  

\section{Data Collection}
% 219 words
First, we collected source codes by crawling \cite[GitHub]{github}, cloning about 900 repositories with C language topic, 
and recursively searched each folder to enumerate C language source code.
From each source code, we generated pairs of C source code fragment and x64 assembly code corresponds to the C fragment.
Some sample fragments are shown in figure~\ref{fig:pairsoffragments}. 

\begin{figure}
	\begin{tabular}{cc}
		(TODO)データ対の例を載せる
	\end{tabular}
	\caption{Sample data fragments}
	\label{fig:pairsoffragments}
\end{figure}

The pairs were generated as follows. Let {\sl S0} be an original C source code.
\begin{enumerate}
\item Remove all preprocessor, such as {\sl \#include}, in {\sl S0} to generate preprosessed C code {\sl S1}. 
\item Tokenize {\sl S1} and rearrange them in {\sl S2} so that there is exactly one token for each line. 
\item 
Compile {\sl S2} by gcc (GNU C compiler) with no optimization (-O0) and debug option to get assembly code {\sl S3}. 
With debug option, {\sl S3} contains information about the source code position which each assembly operations are originated from.
\item Assemble {\sl S3} to object file {\sl S4} and disassemble {\sl S4} to get information eliminated assembly code {\sl S5}.
\item 
Parse {\sl S2} with clang to get AST (abstract syntax tree) of the source code. 
And for each subtree of AST, we can get C fragment source code which corresponds to the subtree, 
and we can get assembly code corresponds to the C fragment by employing {\sl S5} combine with {\sl S3}.
\end{enumerate}

The C fragment and assembly code pairs are preprocessed for the training and testing data.


\section{Data Preprocessing}

% 34 words
In order to apply LSTM, input and output for the LSTM must be the sequence of tokens. 
So we have to preprocess the assembly data and C fragments to the token sequences by somehow.

\subsection{Assembly Code Preprocess}
% 155 words
The assemblies were preprocessed in two ways.
First way is the raw binary integers. The assemblies are finally assembled to the sequence of the one byte integers, ranged from 0 to 255.
So the one byte integer sequences can be directly used for input sequences.
Second way is the tokenized sequence of assembly codes. 
The disassembled opcodes and operands in {\sl S5} are split to symbols, and concatenated the sequences to one sequence, then we modified followings.
\begin{itemize}
\item For each end of the operations, the {\sl ENDLINE} symbol is added to indicate the end of operation.
\item 
The relative access of code address, such as {\sl call} or {\sl jump} operations, 
are relabeled like $\alpha$-conversion for making code position independent and for reducing the vocabulary size.
\item The big integer values are substituted. 
In detail, the integer in the range from -255 to 256 are preserved as it is, and the other integeres are substituted with the {\sl \_\_HEX\_\_} symbol.
This also reduces the vocabulary size.
\end{itemize}


\subsection{C Code Preprocess}
% 232 words
The C source codes were parsed to AST by the \cite{clang} and its Python bindings together with \cite{pycparser}, 
because all of the tools are individualy not sufficient for generating dataset.
Then, the AST is converted to input data by following modifications.

\begin{itemize}
\item The variable-ary nodes and optional nodes are converted. 
For example, the {\sl CompoundStatement} node has variable number of statements. 
We converted such list of nodes to the chains of concatnation and terminal, like a list representation in functional language.
Another example is the  {\sl If} statement, whose {\sl else} clausure are sometimes missing. 
We converted such optional node to the alternative of some node and Null node, like a optional data structure in functional language.
\item The variable names, function names, label names, or type names were renamed like $\alpha$-conversion.
\item There are four type of literals in C language. 
The integer literal less than 256 is preserved, and the literal greater than 256 is clipped to 256.
The string, float and character literals are converted to {\sl \_\_LITERAL\_(string|float|char)\_\_} simbol.

Modifying the node to indicate the value in the assembly code corresponding to the literal is also conceibable way,
but we couldn't implement this because of its technical difficalty, so this is left to future work.
\end{itemize}

The AST is used for sequence-to-Tree model dataset as it is.
Also, the AST is converted to C token sequence for sequence-to-sequence model dataset. 


\section{Model Configuration}
The training model was implemented with Chainer. Source codes are avaiable at \url{}.


(TODO)レイヤ数、ロスについてのべる。

\section{Training and Testing}
% 91 words
We trained the model to reduce the cross-entropy loss between predicted and actual conditional probability.
The conditional probability is the probability of next token $ p(sy_k|sx,[sy_1,\dots,sy_{k-1}]) $ for sequence-to-sequence model, 
and the probability of next action $p(a_{k+1}|sx,[a_1, \dots a_{k}]) $ for sequence-to-tree model.

We calculated the quality of the decompilation by two metrics, edit distance ratio and BLEU.

Edit distance ratio is the metrics for the correctness of the decompilation result.
Here, the edit distance ratio between two strings means the Levenshtein distance divided by the longer length of the strings.
Idealy, the correctness of the decompilation result should be measured by the distance of the meaning of the program.
However the decompilation result is usually uncompilable, so we used the edit distance of the source codes as the approximation of the correctness.

BLEU is 

\chapter{Results}

We tested our methods for 


(TODO)seq2seq、アテンション付きseq2seq、seq2treeとバイナリそのまま/逆アセンブル後 の6パターンのデータを出す。
\chapter{Discussion}
\chapter{Conclusion}

\bibliographystyle{plainnat}
\bibliography{reference}

\end{document}
